{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamed-stifi/PFA-Arabic-LLMs/blob/main/scraping_mawdoo3_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uhfYXQ_nRvG"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08vVB7-TnjmG"
      },
      "source": [
        "**Scraping mawdoo3.com**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXabdqLYYAnZ"
      },
      "outputs": [],
      "source": [
        "def filter_fun(tag):\n",
        "  \"\"\"\n",
        "    Custom filter function to be used with BeautifulSoup find_all.\n",
        "    Filters out script tag.\n",
        "\n",
        "    Parameters:\n",
        "    - tag (bs4.element.Tag): A BeautifulSoup Tag object.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the tag should be included, False otherwise.\n",
        "  \"\"\"\n",
        "  return tag.name != \"script\"\n",
        "\n",
        "def del_attrs(soup):\n",
        "    \"\"\"\n",
        "    Recursively removes all attributes from the given BeautifulSoup tree.\n",
        "\n",
        "    Parameters:\n",
        "    - soup (bs4.BeautifulSoup): A BeautifulSoup object representing the HTML document.\n",
        "    \"\"\"\n",
        "    tags = soup.find_all(True, recursive= False)\n",
        "    if len(tags) > 0:\n",
        "        for tag in tags:\n",
        "            tag.attrs = {}\n",
        "            del_attrs(tag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0NgSEsXfeCl"
      },
      "outputs": [],
      "source": [
        "def mowdoo3_topics(url = 'https://mawdoo3.com'):\n",
        "    \"\"\"\n",
        "    Scrapes and retrieves a list of topic URLs from the Mawdoo3 website.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the Mawdoo3 website. Default is 'https://mawdoo3.com'.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of strings representing the full URLs of topics on Mawdoo3.\n",
        "    \"\"\"\n",
        "\n",
        "    # Send a GET request to the specified URL\n",
        "    res = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    # Find the main section containing the topics\n",
        "    main_section = soup.find_all('section', class_ = 'container home')[0]\n",
        "\n",
        "    # Find the block content within the main section\n",
        "    block_content = main_section.find_all('div', class_ = \"row block-content\")[0]\n",
        "\n",
        "    # Find all 'a' tags within the block content\n",
        "    a_tags = block_content.find_all('a')\n",
        "\n",
        "    # Extract href attributes from 'a' tags and construct full URLs\n",
        "    hrefs = ['https://mawdoo3.com'+tag['href'] for tag in a_tags]\n",
        "\n",
        "    # Return the list of topic URLs\n",
        "    return hrefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Pw49VvhDAr"
      },
      "outputs": [],
      "source": [
        "def articls_of_mawdoo3_topic(topic_url):\n",
        "        \"\"\"\n",
        "        Scrapes and retrieves a list of article URLs for a specific topic on Mawdoo3.\n",
        "\n",
        "        Parameters:\n",
        "        - topic_url (str): The URL of the Mawdoo3 topic page.\n",
        "\n",
        "        Returns:\n",
        "        - list: A list of strings representing the full URLs of articles in the specified topic.\n",
        "        \"\"\"\n",
        "\n",
        "        # Send a GET request to the specified topic URL\n",
        "        res = requests.get(topic_url)\n",
        "\n",
        "        # Parse the HTML content of the topic page using BeautifulSoup\n",
        "        soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "        # Find the content section containing the articles\n",
        "        content = soup.find_all('div', class_ = 'columns large-8 medium-12 small-12')[0].find_all('ul', id = \"grid\")[0]\n",
        "\n",
        "        # Find all 'a' tags within the content section\n",
        "        a_tags = content.find_all('a')\n",
        "\n",
        "        # Extract href attributes from 'a' tags and construct full URLs\n",
        "        articls_urls = ['https://mawdoo3.com'+tag['href'] for tag in a_tags]\n",
        "\n",
        "        # Pagination loop\n",
        "        i = 2\n",
        "        while len(a_tags) == 132:\n",
        "\n",
        "            # Generate the URL for the next page\n",
        "            next_page = topic_url+f'?page={i}'\n",
        "\n",
        "            # Send a GET request to the next page\n",
        "            res = requests.get(next_page)\n",
        "            soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "            # Find the content section on the next page\n",
        "            content = soup.find_all('div', class_ = 'columns large-8 medium-12 small-12')[0].find_all('ul', id = \"grid\")[0]\n",
        "            # Find all 'a' tags on the next page\n",
        "            a_tags = content.find_all('a')\n",
        "\n",
        "            # Break the loop if no more articles are found\n",
        "            if len(a_tags) == 0:\n",
        "                break\n",
        "\n",
        "            # Extract href attributes from 'a' tags on the next page and add to the list\n",
        "            for tag in a_tags:\n",
        "                articls_urls.append('https://mawdoo3.com'+tag['href'])\n",
        "\n",
        "            i = i + 1\n",
        "        return articls_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9ehD1PLwyyz"
      },
      "outputs": [],
      "source": [
        "def get_mawdoo3_articls_urls(url = 'https://mawdoo3.com'):\n",
        "    \"\"\"\n",
        "    Scrapes and retrieves a list of article URLs from Mawdoo3 across all topics.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the Mawdoo3 website. Default is 'https://mawdoo3.com'.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of strings representing the full URLs of articles on Mawdoo3.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store article URLs\n",
        "    aricls_urls = []\n",
        "\n",
        "    # Get a list of URLs for all topics on Mawdoo3\n",
        "    topics_urls = mowdoo3_topics(url)\n",
        "\n",
        "    # Iterate over each topic URL\n",
        "    for ind, topic_url in enumerate(topics_urls):\n",
        "\n",
        "        # Retrieve a list of article URLs for the current topic\n",
        "        topic_articls_urls = articls_of_mawdoo3_topic(topic_url)\n",
        "\n",
        "        # Append each article URL to the overall list\n",
        "        for articl_url in topic_articls_urls :\n",
        "            aricls_urls.append(articl_url)\n",
        "\n",
        "        print('----------- : ', ind)\n",
        "    # Return the list of all article URLs\n",
        "    return aricls_urls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlgzOoNs5x7G"
      },
      "outputs": [],
      "source": [
        "def get_one_articls(url):\n",
        "    \"\"\"\n",
        "    Scrapes and extracts content from a single article on Mawdoo3.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the Mawdoo3 article.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing three elements:\n",
        "      1. titel of the article.\n",
        "      2. Plain text version of the article content.\n",
        "      3. HTML version of the article content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Send a GET request to the specified article URL\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    # Find the article section using the 'article' tag\n",
        "    article = soup.find('article')\n",
        "\n",
        "    # Extract the title of the article\n",
        "    titel = article.h1.get_text(strip=True)\n",
        "\n",
        "    text = article.select_one('[id=\"mw-content-text\"]')\n",
        "\n",
        "    # Extract plain text version of the article content\n",
        "    article_text = text.get_text(strip=True, separator=\"\\n\") if text else \"\"\n",
        "\n",
        "    # Find the index of the substring '\\nالمراجع\\n'\n",
        "    start_index = article_text.find('\\nالمراجع\\n')\n",
        "\n",
        "    # If the substring is found, remove it and everything after it\n",
        "    if start_index != -1:\n",
        "        article_text = article_text[:start_index].strip()\n",
        "\n",
        "    # Find all tags within the article content based on the filter function\n",
        "    text_tags = text.find_all(lambda tag: filter_fun(tag), recursive= False)\n",
        "\n",
        "    # Remove attributes from all tags within the article content\n",
        "    for tag in text_tags:\n",
        "        del_attrs(tag)\n",
        "\n",
        "    # Concatenate the HTML version of the article content\n",
        "    article_html = ''.join(str(text_tags[3:]))\n",
        "\n",
        "    # Find the index of the substring '\\nالمراجع\\n'\n",
        "    start_index = article_html.find('<h2><span><b>المراجع</b></span></h2>')\n",
        "\n",
        "    # If the substring is found, remove it and everything after it\n",
        "    if start_index != -1:\n",
        "        article_html = article_html[:start_index].strip()\n",
        "\n",
        "    return titel, article_text, article_html[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBj4OnTJxJyg"
      },
      "outputs": [],
      "source": [
        "aricls_urls = get_mawdoo3_articls_urls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt6BGRce4qFw",
        "outputId": "9e3c7953-1060-43bd-d22f-ea6ba54ebdde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "77715"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(aricls_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ra0o7xq1voM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "path = '/content/drive/MyDrive/mowdoo3 _dataset/urls_of_articls_of_mawdoo3.json'\n",
        "with open(path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(aricls_urls, json_file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40IX4T6j0K0s"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "path = '/content/drive/MyDrive/mowdoo3 _dataset/urls_of_articls_of_mawdoo3.json'\n",
        "with open(path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    aricles_urls = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIASzvaS3ndz"
      },
      "outputs": [],
      "source": [
        "list_of_articles = []\n",
        "columns = ['article_titel', 'article_text', 'article_html', 'article_url']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pch0pt2V0KxE"
      },
      "outputs": [],
      "source": [
        "for i, article_url in enumerate(aricles_urls):\n",
        "    titel, article_text, article_html = get_one_articls(article_url)\n",
        "    list_of_articles.append([titel, article_text, article_html, article_url])\n",
        "\n",
        "    # print(\"------------------- article -------- : \", i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data1 =list_of_articles[:49765]\n",
        "data2 = list_of_articles[49765:]\n",
        "path1 = \"/content/drive/MyDrive/mowdoo3 _dataset/articls_of_mawdoo3_from_1_to_22748.json\"\n",
        "path2  = \"/content/drive/MyDrive/mowdoo3 _dataset/articls_of_mawdoo3_from_49765_to_77714.json\"\n",
        "with open(path1, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(data1, json_file, ensure_ascii=False, indent=4)\n",
        "with open(path1, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(data2, json_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "mpMK4Yqg818p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISbdIT1u0Kto"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "path1 = \"/content/drive/MyDrive/mowdoo3 _dataset/articls_of_mawdoo3_from_1_to_22748.json\"\n",
        "path2  = \"/content/drive/MyDrive/mowdoo3 _dataset/articls_of_mawdoo3_from_49765_to_77714.json\"\n",
        "with open(path1, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    aricls1 = json.load(json_file)\n",
        "with open(path2, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    aricls2= json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(aricls1['data'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1s5Gs9gdbuk",
        "outputId": "0f0a1f45-e59b-434f-92b9-9cfb82a091a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49765"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY9lfChXnts6"
      },
      "source": [
        "**Scraping islamqa.info**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vO3O_IZqJoK"
      },
      "outputs": [],
      "source": [
        "question_answer_data = {'inputs':[], 'outputs':[], 'urls':[]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbsXWHyonftc"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "urls_path = \"/content/drive/MyDrive/islamqa_dataset/urls_of_islamqa_.json\"\n",
        "with open(urls_path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    loaded_data = json.load(json_file)\n",
        "\n",
        "print(\"length of list : \", len(loaded_data))          #length of list :  36807\n",
        "print(\"length of set : \", len(set(loaded_data)))      #length of set :  29114\n",
        "\n",
        "dataset = list(set(loaded_data))\n",
        "n = len(dataset)\n",
        "end1 = 5000\n",
        "# question_answer_data = {'inputs':[], 'outputs':[], 'urls':[]}\n",
        "for ind, url in enumerate(dataset[:end1]):\n",
        "    question_answer_data['urls'].append(url)\n",
        "    # print(url)\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        question = soup.find_all('section', class_ = 'single_fatwa__question text-justified')[0]\n",
        "        p_tag = question.find('p')          #.get_text()\n",
        "        if p_tag is not None :\n",
        "            question_text = p_tag.get_text()\n",
        "        else:\n",
        "            question_text = question.find('div').get_text()\n",
        "\n",
        "        question_answer_data['inputs'].append(question_text)\n",
        "\n",
        "        answer = soup.find_all('section', class_ = 'single_fatwa__answer')[0].find('div', class_ = 'content')\n",
        "        answer_paragrphs = answer.find_all('p')\n",
        "        full_answer = '\\n'.join([p.get_text() for p in answer_paragrphs])\n",
        "        question_answer_data['outputs'].append(full_answer)\n",
        "    print('---'*4+\" : \" + str(ind) + \" % \", ind/n)\n",
        "question_answer_data_path = f\"/content/drive/MyDrive/islamqa_dataset/question_answer_data_end1_{end1}.json\"\n",
        "with open(question_answer_data_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(question_answer_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "'''\n",
        "was stoe in the next of\n",
        "------------ : 2402 %  0.082503263034966\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17YyOfwC4606ewvR1PY577TLTSWskv3wy",
      "authorship_tag": "ABX9TyN0MRZv7pz3MWHmM9BwCqDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}